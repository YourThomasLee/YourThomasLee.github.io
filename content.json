{"meta":{"title":"Thomas Lee(李柏珍)","subtitle":"抱残守缺","description":"个人博客试验田1号","author":"Thomas Lee (李柏珍)","url":"http://YourThomasLee.github.io","root":"/"},"pages":[],"posts":[{"title":"Notes of paper “mitigating neural network overconfidence with logit normalization”","slug":"Notes of paper “mitigating neural network overconfidence with logit-normalization”","date":"2022-07-11T08:49:33.000Z","updated":"2022-07-12T01:36:44.519Z","comments":true,"path":"2022/07/11/Notes of paper “mitigating neural network overconfidence with logit-normalization”/","link":"","permalink":"http://yourthomaslee.github.io/2022/07/11/Notes%20of%20paper%20%E2%80%9Cmitigating%20neural%20network%20overconfidence%20with%20logit-normalization%E2%80%9D/","excerpt":"introduce Logit Normalization (LogitNorm), a simple fix to the cross-entropy loss (by enforcing a constant vector norm on the logits in training), to deal with overfitting or overconfidence for both in- and out-of-distribution inputs.","text":"introduce Logit Normalization (LogitNorm), a simple fix to the cross-entropy loss (by enforcing a constant vector norm on the logits in training), to deal with overfitting or overconfidence for both in- and out-of-distribution inputs. Problem: we find that even when most training examples are classified to their correct labels, the softmax cross-entropy loss can continue to increase the magnitude of the logit vectors. The growing magnitude during training thus leads to the overconfidence issue, despite having no improvement on the classification accuracy. Work: To mitigate the issue, our key idea behind LogitNorm is to decouple the influence of output’s norm from the training objective and its optimization. This can be achieved by normalizing the logit vector to have a constant norm dur- ing training. In effect, our LogitNorm loss encourages the direction of the logit output to be consistent with the corre- sponding one-hot label, without exacerbating the magnitude of the output. Trained with normalized outputs, the network tends to give conservative predictions and results in strong separability of softmax confidence scores between ID and OOD inputs Codes: https://github.com/hongxin001/logitnorm_ood Core equation: to make sure that the logit vector is a unit vector, it alleviate some challenges in optimization, or rather, let optimization does more things useful！ where is logit vector, is the Euclidean norm of the logit vector, and denotes the quantity of different class labels. Comment: This work restricts the numerical bound of logit vector during optimization, and it results better model performance in classification task. Exploring problem： Can we get the better performance if we use label smoothing? what is the difference between LayerNorm and LogitNorm? Is there any other efficient way to implement LogitNorm? Is this work compatible to the paper “Long-Tail Learning via Logit Adjustment” (https://arxiv.org/pdf/2007.07314.pdf)?","categories":[],"tags":[{"name":"overfitting","slug":"overfitting","permalink":"http://yourthomaslee.github.io/tags/overfitting/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-03-20T15:33:35.180Z","updated":"2022-03-20T15:33:34.000Z","comments":true,"path":"2022/03/20/hello-world/","link":"","permalink":"http://yourthomaslee.github.io/2022/03/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"overfitting","slug":"overfitting","permalink":"http://yourthomaslee.github.io/tags/overfitting/"}]}