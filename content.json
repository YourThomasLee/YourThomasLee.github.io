{"meta":{"title":"Thomas Lee(李柏珍,Baizhen Li)","subtitle":"抱残守缺","description":"个人博客试验田1号","author":"Thomas Lee (李柏珍,Baizhen Li)","url":"http://YourThomasLee.github.io","root":"/"},"pages":[],"posts":[{"title":"Deep learning foundation 0 - framework","slug":"Deep learning 0 - framework","date":"2023-02-20T02:40:05.000Z","updated":"2023-02-10T16:30:04.182Z","comments":true,"path":"2023/02/20/Deep learning 0 - framework/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/20/Deep%20learning%200%20-%20framework/","excerpt":"","text":"","categories":[],"tags":[{"name":"deep learning framework","slug":"deep-learning-framework","permalink":"http://yourthomaslee.github.io/tags/deep-learning-framework/"}]},{"title":"Machine learning foundation 1 - Part 6. Markov chain","slug":"Machine learning foundation 1.5 - Markov chain","date":"2023-02-16T02:40:05.000Z","updated":"2023-02-10T16:34:32.488Z","comments":true,"path":"2023/02/16/Machine learning foundation 1.5 - Markov chain/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/16/Machine%20learning%20foundation%201.5%20-%20Markov%20chain/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 Outline","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"Markov chain","slug":"Markov-chain","permalink":"http://yourthomaslee.github.io/tags/Markov-chain/"}]},{"title":"Machine learning foundation 1 - Part 7. Condition random field","slug":"Machine learning foundation 1.6 - condition random field","date":"2023-02-16T02:40:05.000Z","updated":"2023-02-10T16:35:02.755Z","comments":true,"path":"2023/02/16/Machine learning foundation 1.6 - condition random field/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/16/Machine%20learning%20foundation%201.6%20-%20condition%20random%20field/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 Outline","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"Condition random field","slug":"Condition-random-field","permalink":"http://yourthomaslee.github.io/tags/Condition-random-field/"}]},{"title":"Machine learning foundation 1 - Part 5. Model enhancement","slug":"Machine learning foundation 1.4 - model enhancement","date":"2023-02-12T02:40:05.000Z","updated":"2023-02-10T16:33:53.731Z","comments":true,"path":"2023/02/12/Machine learning foundation 1.4 - model enhancement/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/12/Machine%20learning%20foundation%201.4%20-%20model%20enhancement/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 [TOC] 1. 提升方法概述 在概率近似正确(probably approximately correct, PAC)框架中，一个概念，如果存在一个多项式的学习算法能够学习它，并且学习的正确率很高，则称该概念是强可学习的；如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称该概念是弱可学习的。 事实上，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 提升方法是从弱学习算法出发，得到一系列弱分类器，然后组合这些弱分类器构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布(训练数据的样本权重)，针对不同的训练数据分布调用共弱学习算法学习一系列的弱分类器。 对于提升方法来说，有两个非常重要的问题需要回答： 在每一轮次如何调整训练数据的权重或概率分布 如何将弱分类器组合称一个强分类器 目前典型的提升方法有adaboost，gbdt，xgboost， light gbm，随机森林等等，以下分点阐述这些算法 2. ada-boost算法 AdaBoost(Adaptive Boosting)算法是一个针对二分类的动态自适应提升方法，该方法是顺序训练M个分类器，每个分类器的样本权重根据上一个分类器的分类正确率进行调整得到，每个分类器的权重由分类器的正确率计算得到，将所有分类器的输出进行线性加权结果输入到sign函数输出预测值。 **优点:**1. 非常容易训练，实现起来比较容易; 2. 泛化错误率低（预测性能好），不易过拟合; 3. 不需要调节很多参数，最多修改一下基础模型的数量; 4. 适用范围广：二分类问题，多分类问题，回归问题 **缺点:**1. 对于离群值比较敏感 算法 2.1 adaboost算法 输入： 输出：最终分类器. Step 1. 初始化训练数据的权值分布. Step 2. 对: Step 2.1. 使用具有权值分布的训练数据集学习，得到基本分类器. Step 2.2. 计算在训练数据集上的分类误差率, 如果, 跳转到Step 2.1，否则往下执行Step 2.3. Step 2.3. 计算的权重系数， Step 2.4. 更新训练数据集的权值分布，, 其中为规范化因子，. Step 3. 构建基本分类器的线性组合，, 得到最终分类器，. 每个分类器权重：， 每个样本的权重更新公式，对于正确样本，. **AdaBoost-前向分步算法理解：**考虑加法模型(additive model), 其中为基函数，的参数，为基函数的系数。在给定训练数据和损失函数的条件下，学习加法模型成为经验风险最小化问题. 前向分布算法(forward stagewise algorithm)求解以上优化问题的想法是：从前完后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，具体地每步秩序优化如下函数. 算法 2.2. 前向分步算法 输入： 输出：加法模型. Step 1. 初始化 Step 2. 对 Step 2.1 极小化损失函数,得到参数. Step 2.2 更新. Step 3. 得到加法模型. 设定损失函数为，并从前往后分步优化，设定权重参数, 令权重参数更新方式为(TODO:补充权重更新细节). 第步则为, 由于前个分类器已经确定，只剩第个分类器待优化，故优化目标可写为. 在ada boost上进行基分类器的拓展，例如将ada boost中的基分类器替换成二分类树，这可以认为是ada boost的简单拓展；对于回归问题，使用决策树将空间区域划分为若干不相交的区域(每个叶子节点就是对应着一个区域)，而后对于每个区域进行线性回归，则可得提升树的回归算法(残差树)。 3. gbdt算法 gbdt是使用加法模型并使用前向分布算法不断减小训练过程中产生的残差估计值来达到将数据分类或者回归的提升算法，相比于adaboost和残差树，gbdt考虑更加泛化的损失函数情况，因此基于梯度的残差估计值来训练每一个轮次的弱分类器，具有更加广泛的适用性。让损失函数沿着梯度方向的下降，这是gbdt的gb的核心了。 gbdt=前向分布+梯度提升+cart 回归树 gbdt训练：前向分布训练个分类器(贪心算法)，第一次直接基于原始数据集训练，而后使用 gbdt分类树：cart回归树预测数值，之后增加一层softmax 4. xgboost算法 使用二阶泰勒公式来近似估计残差，在训练时候加入正则化项避免 5. light gbm算法 #TODO 6. 随机森林 直觉： 随机森林是采用有放回采样样本和随机采样特征形成若干个决策树分类器投票预测最终的样本标签值的集成学习方法 优点： 模型较低的方差，较好的泛化性能； 由于随机采样特征，也可以作为特征选择的方法； 由于多个分类器之间独立不相关，并行性能较好，速度较快； 缺点：投票决策导致不能够预测从来没有出现过的标签值，同时预测值的空间为离散的，这导致在回归问题上表现较差；由于没有考虑特征之间的相关性，在有些情况下随机森林的模型效果可能不尽人意 6.1. bagging 给定大小为的样本集合，使用有放回采样方构造个样本集(每个样本集个样例)，使用个样本集训练不同的分类或者回归预测模型，并使用简单投票或者算术平均值求预测值。 6.2. 随机森林 随机森林是 Bagging 的优化版本。其包含的思想在于： 随机选择样本数建立多个训练集并随机选取特征集合，根据多个训练集与特征集合来建立多颗决策树，然后进行投票决策。 随机森林的最终目的是建立颗决策树，而每颗决策树的建立过程如下： 如果训练集大小为，对于每棵树而言，随机且有放回地从训练集中的抽取个训练样本，作为该树的训练集。 如果每个样本的特征维度为，指定一个常数，随机地从个特征中选取个特征子集，每次树进行分裂时，从这个特征中选择最优的 每棵树都尽最大程度的生长，并且没有剪枝过程。 随机森林中的随性性指的是：数据采样的随机性与特征采用的随机性。 这两个随机性的引入对随机森林的分类性能直观重要，它们使得随机森林不容易陷入过拟合，且具有很好的抗噪能力。是随机森林中唯一的一个参数。 减小特征选择个数，树的相关性和分类能力也会相应的降低 增大，两者也会随之增大。 6.3. 如何处理缺失值 首先给缺失值预设一些估计值，例如平均数，中位数，众数； 根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径. 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有组数据，相似度矩阵大小就是。如果缺失值是类别变量，通过投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。 6.4. 什么是 OOB？ OOB 即 out-of-bag ， 又称袋外数据。 这是由于 Bagging 方法会采用 Boostrap 进行抽样， 每次约有 的样本不会出现在抽样后的样本集合中，那么就把这 的样本称为袋外数据 oob(out-of-bag)。由于 oob 没有用于训练决策树，因此可用于后续对该决策树的泛化能力评估。 6.5. 随机森林的优劣 优势: 模型高度并行化, 对于高维数据集的处理能力比较好, 可以处理成千上万的输入变量,并确定出重要的变量, 因此也被认为是不错的降维方法. 能够处理分类和回归两种类型的问题, 表现良好, 由于是集成学习, 方差和偏差都比较小, 泛化性能优越. 对于数据的鲁棒性很强, 可以处理缺失数据/不平衡数据, 无需归一化 劣势: 随机森林在回归问题上较为弱势, 这是因为它不能给出一个连续型的输出.另外随机森林不能够做出超越训练数据集范围的预测, 这可能导致在有特定噪声的数据进行建模时出现过度拟合 黑盒模型, 无法控制模型运行 随机森林模型忽略了属性之间的相关性","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"model enhancement","slug":"model-enhancement","permalink":"http://yourthomaslee.github.io/tags/model-enhancement/"}]},{"title":"Machine learning foundation 1 - Part 3. Perception machine and support vector machine","slug":"Machine learning foundation 1.2 - perception machine and support vector machine","date":"2023-02-11T02:40:05.000Z","updated":"2023-02-10T16:32:51.062Z","comments":true,"path":"2023/02/11/Machine learning foundation 1.2 - perception machine and support vector machine/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/11/Machine%20learning%20foundation%201.2%20-%20perception%20machine%20and%20support%20vector%20machine/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 Outline","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"perception machine","slug":"perception-machine","permalink":"http://yourthomaslee.github.io/tags/perception-machine/"},{"name":"support vector machine","slug":"support-vector-machine","permalink":"http://yourthomaslee.github.io/tags/support-vector-machine/"}]},{"title":"Machine learning foundation 1 - Part 4. decision tree","slug":"Machine learning foundation 1.3 - decision tree","date":"2023-02-11T02:40:05.000Z","updated":"2023-02-10T16:33:04.743Z","comments":true,"path":"2023/02/11/Machine learning foundation 1.3 - decision tree/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/11/Machine%20learning%20foundation%201.3%20-%20decision%20tree/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 [TOC] 1. 预备知识 1.1 基础数学知识： 期望. 另外如果与独立时. 方差： 方差是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。 方差是和中心偏离的程度，用来衡量一批数据的波动大小（即这批数据偏离平均数的大小）并把它叫做这组数据的方差。 . . 如果两个随机变量相互独立，. 标准差 协方差：,相关系数. 特征相关性分析：在统计学中，皮尔逊相关系数( Pearson correlation coefficient），又称皮尔逊积矩相关系数（Pearson product-moment correlation coefficient，简称 PPMCC或PCCs），是用于度量两个变量X和Y之间的相关(线性相关)，其值介于-1与1之间。皮尔逊相关系数有一个重要的数学特性是，因两个变量的位置和尺度的变化并不会引起该系数的改变，即它该变化的不变量移动到和把Y移动到，其中a、b、c和d是常数，并不会改变两个变量的相关系数（该结论在总体和样本皮尔逊相关系数中都成立）。 假设划分前样本集合D的熵为 。使用某个特征A划分数据集D，计算划分后的数据子集的熵为 。 信息熵：条件熵：信息增益： 1.2 决策树基础 目前典型的决策树算法有三种，分别是ID3、C4.5和CART算法。 算法 划分标准 ID3 信息增益 C4.5 信息增益率 CART 基尼系数 决策树的三要素 特征选择： 从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。 **决策树生成：**根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 **决策树的修剪：**决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。 在决策树的生成和修剪步骤，对于三个算法大同小异，下面主要学习下特征的选择问题。 决策树算法的框架如下所示 algorithm 2.3.1 决策算法 输入：训练集, 属性集 输出:一棵决策树 函数： 123456789生成节点node;if D中样本全部属于同一类别C then 将node节点标记为C类叶子节点, return ;if A为空集 or D中样本在A上取值相同 then 将node节点标记为叶节点,其类别标记为D中样本数最多的类 return ;从A中选择最优划分属性a_max;for a_max中的每一个取值a_val: 为node生成一个分支;令D_v表示D中取值为a_val的样本子集 以TreeGenerate(D_v,A-{a_max})为分支节点 因此决策树的关键在于选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能的属于同一类别。 2. 特征选择标准 2.1. 信息增益(ID3算法) 设当前样本集合D中第类样本所占的比例为 那么D的信息熵定义为： 使用属性作为划分属性的信息增益为 对取值数目较多的属性有偏好, 其根本原因在于, 当时， 最大， 从公式建模的角度上来说，信息熵建模本身就无法提供任意两个概率分布之间的距离估算。 2.2. 增益率(C4.5算法) 增益率定义为 被称为的“固有值”。需要注意的是，增益率准则对可取值数目较少的属性有所偏好。因此，C4.5算法并不是直接选择增益率最大的侯选划分属性，而是使用了一个启发式：先从侯选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的, 但从根本上来说，由于仍然使用信息熵作为基础的评估方式，启发式方法只能一定程度上缓解对多值属性的偏好。 2.3. 基尼指数(CART树) 对于离散数据，数据集的纯度可以使用基尼值来度量，值的定义如下： 直观来说，反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。值越小，则数据集的纯度越高。属性的基尼指数定义为： 于是我们在侯选属性集合A中选择那个使得划分后基尼指数最小的属性作为最优划分属性，即. 对于回归任务，一颗回归树对应着输入空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为个单元,并且在每个单元上游一个固定的输出值, 回归树模型可以表示为, 当输入空间的划分确定后，使用平方误差来表示回归树在训练数据的预测误差，并求解单元上的的最优值，易知所有值的最优值为上样本点标签的均值。 3. 树生成 3.1. ID3算法 算法 3.1. ID3树生成算法 输入: 训练数据集，特征集阈值. 输出: 决策树. Step 1. 如果中的所有实例都属于同一类, 则为单节点树，并将类作为节点的标记, 返回； Step 2. 如果, 则为单节点树，并将数据集中实例树最大的类作为节点的标记, 返回; Step 3. 计算所有属性对决策属性的信息增益，选择信息增益最大的属性; Step 4. 如果的信息增益小于阈值, 则置为单节点树，并将数据集中实例树最大的类作为节点的标记, 返回; Step 5. 根据的取值获得数据的一个划分，对划分中的每一个子数据集为训练集作为特征集，递归调用Step 1-5生成叶子节点的类别标记. 3.2. C4.5算法 算法 3.1. C4.5树生成算法 输入: 训练数据集，特征集阈值. 输出: 决策树. Step 1. 如果中的所有实例都属于同一类, 则为单节点树，并将类作为节点的标记, 返回； Step 2. 如果, 则为单节点树，并将数据集中实例树最大的类作为节点的标记, 返回; Step 3. 计算所有属性对决策属性的信息增益率，选择信息增益率最大的属性; Step 4. 如果的信息增益小于阈值, 则置为单节点树，并将数据集中实例树最大的类作为节点的标记, 返回; Step 5. 根据的取值获得数据的一个划分，对划分中的每一个子数据集为训练集作为特征集，递归调用Step 1-5生成叶子节点的类别标记. 3.3. CART算法 算法 3.3.1. 最小二乘回归算法 输入: 训练数据集; 输出：回归树 Step 1. 选择最优切分变量和切分点,求解 遍历变量，对固定的切分变量扫描切分点, 选择使上式最小的; Step 2. 用选定的划分区域并决定相应的输出值: Step 3. 继续对两个子区域调用上述步骤，直到满足停止条件 Step 4. 将输入控件划分为个子区域, 生成决策树. 算法 3.3.2 CART分类树 输入: 训练数据集，停止计算的条件. 输出: 决策树. Step 1. 如果中的所有实例都属于同一类, 则为单节点树，并将类作为节点的标记, 返回； Step 2. 如果, 则为单节点树，并将数据集中实例树最大的类作为节点的标记, 返回; Step 3. 计算所有属性对决策属性的Gini指数(对所有属性的属性值基于是否划分为两个数据集子集)，选择Gini指数最小的属性; Step 4. 根据的最优值划分获得数据的一个划分，对划分中的每一个子数据集为训练集作为特征集，递归调用Step 1-4生成叶子节点的类别标记. 4. 剪枝处理 4.1. 预剪枝 将数据集划分为训练集和测试集，每次分支也考虑验证集上的分类效果，在西瓜书中其评价指标为分类精度，若分支后的分类精度在训练集上和测试集上都有提升，那么分支，否则禁止分支。 4.2.后剪枝 以决策树生成节点的顺序为节点的序，逆序遍历所有分支节点，考察替换成叶子节点后验证集上分类精度的变化，若有提升则将分支节点替换成叶子节点； 另外一种情况是使用损失函数来进行剪枝与否判断。设树的叶节点个数为, 是树的叶子节点，该叶节点有个样本点，其中类的样本点有个，. 是叶节点上的经验熵，则决策树学习的损失函数可以定义为 , 对所有叶子节点确定剪枝前后的损失函数值来确定。 算法 4.2.1 CART剪枝算法 输入：CART算法生成的决策树 输出：最优决策树 Step 1. 设, , Step 2. 自下而上地对各内部节点计算以及,其中表示以为根节点的子树，是对训练数据的预测误差，是树的叶节点个数。 5. 连续值与缺失值处理 5.1. 连续值处理 设存在连续属性，假定出现了个不同取值，将这些值从小到大排序，记为。记由划分点得到的数据划分为。因此，对连续属性可以考察个元素的侯选划分点集合 而后考察划分。而后计算得到的信息增益可用以分支判断。需要注意的是，分支每次只分两支，也就是说离散化每次分支只进行一次，后续分支仍可继续使用该属性继续离散化分支。 5.2. 缺失值处理 使用已知数据来评估分支，而后将缺失值的对象加入到每一个拓展分支的对象集中. 例如, 设存在缺失值的属性, 表示属性上无缺失值的样本, 表示无缺失值样本所占比例, 表示无缺失值样本第类样本所占比例. 表示无缺失值样本中上取值的样本的比例. 则可将信息增益的计算式推广为: 6. 拓展 6.1. 多变量决策树 与传统的单变量决策树不同, 多变量决策树学习过程中, 不是为每个非叶节点寻找一个最优划分属性, 而是试图建立一个合适的线性分类器. 多变量决策树算法主要有OC1[Murthy et al., 1994]和[Brodley and Utgoff, 1995]提出的一系列算法. OC1先贪心地寻找每个属性的最优权值, 在局部优化的基础上在对分类边界进行随机扰动以试图寻找到更好的边界; [Brodley and Utgoff, 1995]则直接引入了线性分类器的最小二乘法. 6.2. 为何信息增益会偏向多取值特征？ 从直观的理解上来说，当特征取值较多时， 根据此特征划分得到的子集纯度有更大的可能性会更高（对比取值较少的特征）， 因此划分之后的熵会更低，而又由于划分之前的熵是一定的，因此信息增益更大。 6.3. 树形结构为何不需要归一化？ 因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"logic regression","slug":"logic-regression","permalink":"http://yourthomaslee.github.io/tags/logic-regression/"}]},{"title":"1223. Dice Roll Simulation","slug":"Leetcode 1223","date":"2023-02-10T02:40:05.000Z","updated":"2023-02-10T03:16:52.183Z","comments":true,"path":"2023/02/10/Leetcode 1223/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/10/Leetcode%201223/","excerpt":"","text":"The problem description can be found at the link Solution explanation can be referred at link Solution: 123456789101112131415161718192021class Solution {public: static constexpr int mod = 1E9 + 7; int dieSimulator(int n, vector&lt;int&gt;&amp; rollMax) { vector d(n + 1, vector&lt;int&gt;(6, 0)); vector&lt;int&gt; sum(n + 1, 0); sum[0] = 1; for (int i = 1; i &lt;= n; i++) { for (int j = 0; j &lt; 6; j++) { int pos = max(i - rollMax[j] - 1, 0); int sub = ((sum[pos] - d[pos][j]) % mod + mod) % mod; d[i][j] = ((sum[i - 1] - sub) % mod + mod) % mod; if (i &lt;= rollMax[j]) { d[i][j] = (d[i][j] + 1) % mod; } sum[i] = (sum[i] + d[i][j]) % mod; } } return sum[n]; }};","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yourthomaslee.github.io/tags/leetcode/"},{"name":"dynamic programming","slug":"dynamic-programming","permalink":"http://yourthomaslee.github.io/tags/dynamic-programming/"},{"name":"combination","slug":"combination","permalink":"http://yourthomaslee.github.io/tags/combination/"},{"name":"interesting problems","slug":"interesting-problems","permalink":"http://yourthomaslee.github.io/tags/interesting-problems/"}]},{"title":"Machine learning foundation 1 - Part 2. Logic regression and maximum entropy models","slug":"Machine learning foundation 1.1 - logic regression and maximum entropy model","date":"2023-02-10T02:40:05.000Z","updated":"2023-02-10T16:32:23.107Z","comments":true,"path":"2023/02/10/Machine learning foundation 1.1 - logic regression and maximum entropy model/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/10/Machine%20learning%20foundation%201.1%20-%20logic%20regression%20and%20maximum%20entropy%20model/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 Outline [TOC] 1. 最大熵原理 熵§是一个评估信息（也叫不确定性）的一个函数，当分布趋向于均匀分布的时候，熵达到最大。 最大熵原理认为，学习概率模型时，在满足约束条件的情况下（拟合训练数据），在所有可能得概率模型(分布)中，熵最大的模型是最好的模型（对所有未知情况保持平等的态度，也就是等可能）。 1.1 最大熵模型的定义 我们考虑具体的一个学习任务，假设任务已经被形式化成以下函数","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"logic regression","slug":"logic-regression","permalink":"http://yourthomaslee.github.io/tags/logic-regression/"},{"name":"maximum entropy model","slug":"maximum-entropy-model","permalink":"http://yourthomaslee.github.io/tags/maximum-entropy-model/"}]},{"title":"1797. Design Authentication Manager","slug":"Leetcode 1797","date":"2023-02-09T02:40:05.000Z","updated":"2023-02-09T02:37:57.470Z","comments":true,"path":"2023/02/09/Leetcode 1797/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/09/Leetcode%201797/","excerpt":"","text":"The problem description can be found at the link My solution: 12345678910111213141516171819202122232425class AuthenticationManager {public: int timeToLive; map&lt;string, int&gt; u2t; AuthenticationManager(int timeToLive) { this-&gt;timeToLive = timeToLive; } void generate(string tokenId, int currentTime) { u2t[tokenId] = currentTime; } void renew(string tokenId, int currentTime) { if(u2t.find(tokenId) != u2t.end() &amp;&amp; currentTime - u2t[tokenId] &lt; timeToLive) u2t[tokenId] = currentTime; } int countUnexpiredTokens(int currentTime) { int ans = 0; for(auto x: u2t){ if(currentTime - x.second &lt; timeToLive) ans++; } return ans; }};","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yourthomaslee.github.io/tags/leetcode/"},{"name":"hash","slug":"hash","permalink":"http://yourthomaslee.github.io/tags/hash/"}]},{"title":"Deep learning foundation 0 - framework","slug":"Deep learning 1 - activation function","date":"2023-02-08T02:40:05.000Z","updated":"2023-02-10T16:30:27.962Z","comments":true,"path":"2023/02/08/Deep learning 1 - activation function/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/08/Deep%20learning%201%20-%20activation%20function/","excerpt":"","text":"Activation function 激活函数的根本目的是增加非线性，能够通过梯度下降有选择的观察某一个区域的特征 1. Sigmoid 公式​ 优点：良好的生物解释，从完全不激活(0)到完全饱和的激活(1)，可以直接映射为概率 缺点： 梯度消失(初始化小于1)，当神经元的激活在接近0或1出时会饱和，梯度几乎为0，为了防止饱和，必须对初始化权重留意，不宜过大，否则容易梯度消失网络就不学习了. 梯度爆炸（初始化&gt;1)， sigmoid导数最大为1/4，所以时候才可能出现梯度爆炸 非0均值：因为输出都为正，对下一层求导的局部梯度也是正，因为，会造成捆绑效应 2. Tanh 公式, 将数值区域从[0,1]扩张大[-1,+1]区间，输出零中心，增快收敛。但仍然有幂函数，计算成本高 3. Relu 公式. 在正区间解决了梯度消失的问题，快收敛，计算成本低。缺点是对学习率很敏感，太大会出现很多死神经元。 4. Leaky Relu 公式. 分段逼近，快速收敛，但效果不稳定。解决死亡神经元的尝试 5. Maxout 公式, 具备relu所有的优点，没有死神经元。缺点是参数量增加了一倍。 6. GELU Bert中使用的激活函数","categories":[],"tags":[{"name":"deep learning concepts","slug":"deep-learning-concepts","permalink":"http://yourthomaslee.github.io/tags/deep-learning-concepts/"},{"name":"activation function","slug":"activation-function","permalink":"http://yourthomaslee.github.io/tags/activation-function/"}]},{"title":"1233. Remove Sub-Folders from the Filesystem","slug":"Leetcode 1233","date":"2023-02-08T02:40:05.000Z","updated":"2023-02-08T01:23:00.267Z","comments":true,"path":"2023/02/08/Leetcode 1233/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/08/Leetcode%201233/","excerpt":"","text":"Problem description can be found at link My solution: 12345678910111213141516171819class Solution {public: vector&lt;string&gt; removeSubfolders(vector&lt;string&gt;&amp; folder) { sort(folder.begin(), folder.end()); vector&lt;string&gt; ans; int p = 0, len = 0; ans.emplace_back(folder[0]); for(int i = 1; i &lt; folder.size(); ++i){ len = min(folder[i].size(), folder[p].size()); if(folder[i].substr(0, len) == folder[p].substr(0, len) &amp;&amp; folder[i][len] == '/'){ continue; }else{ ans.emplace_back(folder[i]); p = i; } } return ans; }};","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yourthomaslee.github.io/tags/leetcode/"},{"name":"array","slug":"array","permalink":"http://yourthomaslee.github.io/tags/array/"}]},{"title":"Machine learning foundation 0 - framework","slug":"Machine learning foundation 0 - framework","date":"2023-02-08T02:40:05.000Z","updated":"2023-02-10T04:31:35.776Z","comments":true,"path":"2023/02/08/Machine learning foundation 0 - framework/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/08/Machine%20learning%20foundation%200%20-%20framework/","excerpt":"","text":"基础概念 任务：给定一定量的数据，利用模型归纳、总结、学习数据中的规律和知识，应用到新数据的观察、评估、预测中 学习：如果一个系统能够通过执行某个过程改进它的性能， 这就是学习（Herbert A. Simon） 统计学习：从给定的、有限的、用于学习的训练数据集合(training data)出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间(hypothesis space); 应用某个评价准则(evaluation criterion)，从假设空间中选取一个最优模型，使它对已知的训练数据及位置的测试数据在给定评价准则下有最优的预测；最优模型的选取由算法实现。 其重要性在于（1）可以处理海量数据；（2）被证明是计算机智能化的有效手段；（3）是计算机学科发展的重要组成部分； 统计学习分类：从任务上来说，一般包括监督学习、无监督学习、强化学习。有时还包括半监督学习、主动学习；[李航， 统计学习方法(2rd)] 标注数据表示输入输出的对应关系，预测模型对给定的输入产生相应的输出。监 督学习的本质是学习输入到输出的映射的统计规律。 无监督学习是指从无标注数据中学习预测模型的机器 学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。无 监 督 学 习的 本 质 是 学 习 数 据 中 的 统 计规 律 或 潜 在 结 构 强化学习(reinforcement learning)是指智能系统在与环境的连续互动中学习 最优行为策略的机器学 习问题。假设智能 系统与环境的互动基于马尔可 夫决策过 程(Markov decision process)，智能系统能观测到的是与环境互动得到的数据序列。 强化学习的本质是学习最优的序贯决策。强化学习的目标就是在所有可能的策略中选出价值函数最大的策略，而在实际 学习中往往从具体的策略出发，不断优化己有策略。 半监督学习(semi-supervised learning) 是指利用标注数据和未标注数据学习预 测模型的机器学 习问题。通常有少量标注数据、大量未标注数据，因为标注数据的构 建 往 往 需 要 人 工， 成 本 较 高 ， 未 标 注 数 据 的 收 集 不 需 太 多 成 本 。 半 监 督 学 习 旨 在 利 用 未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习 效果。 主动学习(active learning)是指机器不断主动给出实例让教师进行标注，然后利 用标注数据学习预测模型的机器学习问题。通常的监督学习使用给定的标注数据，往 往是随机得到的，可以看作是“ 被动学习”，主动学习的目标是找出对学习最有帮助的 实例让教师标注，以较小的标注代价，达到较好的学习效果。 从模型的角度上说，可以分为 概率模型（决策树、朴素贝叶斯、隐马尔科夫模型、条件随机场、概率潜在语义分析、高斯混合模型等）和非概率模型（感知机、支持向量机、近邻、AdaBoost、均值聚类、潜在语义分析以及神经网络）； 线性模型（感知机、线性支持向量机、近邻、均值聚类、潜在语义分析）和非线性模型（核支持向量机、AdaBoost、神经网络）； 参数化模型（感知机、朴素贝叶斯、逻辑回归、均值聚类、高斯混合模型等等）和非参数化模型（决策树、支持向量机、AdaBoos、近邻、语义分析和狄利克雷分配） 按算法分类则可以分为在线学习(online learning)和批量学习(batch learning); 按技巧分类则有贝叶斯学习（其 主 要 想 法 是 ， 在 概 率模 型 的 学 习 和 推 理 中 ， 利 用 贝 叶斯定理，计算在给定数据条件 下模型的条件概率，即后验概率，并应用这个原理进 行模型的估计，以及对数据的预测。将模型、未观测要素及其参数用变量表示，使用 模型的先验分布是贝叶斯学习的特点）、核方法（使用核函数表示和学习非线性模型的一种机器学习方 法）。 统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下： 得到一个有限的训练数据集合； 确定包含所有可能的模型的假设空间，即学习模型的集合； 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）； 实现求解最优模型的算法，即学习的算法（优化算法）； 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）； 利用学习的最优模型对新数据进行预测或分析； [由于李航的泛化下界分析内容没有写成闭包，此处暂时不进行深入学习，后续再复习] 后续将从两个维度进行学习，第一个是机器学习常见模型进行学习，这一部分更加侧重理论，第二个是按实现步骤进行学习，更加侧重应用和思考 有空可以后续学习的信息： 机器学习理论： Guarantees in Machine learning pen-and-paper exercises in machine learning： https://github.com/michaelgutmann/ml-pen-and-paper-exercises","categories":[],"tags":[{"name":"machine learning framework","slug":"machine-learning-framework","permalink":"http://yourthomaslee.github.io/tags/machine-learning-framework/"}]},{"title":"Machine learning foundation 1 - Part 1. linear regression","slug":"Machine learning foundation 1.0 - linear regression","date":"2023-02-08T02:40:05.000Z","updated":"2023-02-10T04:33:43.374Z","comments":true,"path":"2023/02/08/Machine learning foundation 1.0 - linear regression/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/08/Machine%20learning%20foundation%201.0%20-%20linear%20regression/","excerpt":"","text":"12345678统计学习方法三大要素：模型（model)、策略（strategy)和算法（algorithm）。其一般的实现步骤如下：1. 得到一个有限的训练数据集合；2. 确定包含所有可能的模型的假设空间，即学习模型的集合；3. 确定模型选择的准则，即学习的策略（一般经验风险最小化[各种损失函数]+结构风险最小化[L1正则化、L2正则化]）；4. 实现求解最优模型的算法，即学习的算法（优化算法）；5. 通过学习方法选择最优的模型（简单交叉、K折交叉、留1交叉验证）；6. 利用学习的最优模型对新数据进行预测或分析；后续将对一个个模型进行深入的学习和理解 Outline 1234567891011121314graph LRA(线性回归) --&gt; B[直觉]A(线性回归) --&gt; C[模型]A(线性回归) --&gt; D[优化]A(线性回归) --&gt; E[拓展]B[直觉] --&gt; F[找到一条直线或一个平面能够根据输入的特征向量来更好的预测输出y的值]B[直觉] --&gt; G[损失函数:平方损失,绝对值损失,huber损失]C[模型] --&gt; N[线性函数]D[优化] --&gt; H[梯度下降]D[优化] --&gt; I[正规方程组]E[拓展] --&gt; J[Lasso回归-L1 Norm]E[拓展] --&gt; K[岭回归-L2 Norm]E[拓展] --&gt; L[局部加权线性回归]E[拓展] --&gt; M[ElasticNet-L1 L2 Norm] 1. 直觉与目标 简单来说，线性回归算法就是**找到一条直线（一元线性回归）或一个平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。**其本质含义在于 X 与 Y 是线性相关的。 在有了该直觉之后，我们的第一个目标是根据给定的如何评估一条直线是否合理，也就是如何评估出多个不同的直线的序，目前比较典型的思路是设计一个函数，输入给定的模型和数据，输出键值，比较键值大小就可以知晓哪一条直线是最优的(一般默认越小越好)。就目前来说，典型的键值(损失)函数有三种，具体公式如下，损失函数图如下。 平方损失： , 对异常点有较大的惩罚，不够robust。 绝对值损失：，在0处不可导，不易优化 Huber损失：，该损失函数是1,2的综合，较为鲁棒 第一个目标达成后，我们的目标就变为根据数据将不合理的直线调整变换为合理，该过程也叫学习，在机器学习中叫优化, 该部分内容详见第3部分。 2. 模型 给定训练数据, 线性回归的模型设定为 一般地，我们使用平方损失函数来做为衡量直线优劣的指标，也即. 朴素的二元线性回归：梯度下降法 Missing \\end{aligned} \\begin{aligned}\\frac{\\partial L}{\\partial w}=&amp;\\sum_{i=1}^{n}2(wx_i+b-y_i)x_i\\&amp;=2\\sum_{i=1}^{n}w(x_i)^2+bx_i-x_iy_i\\\\frac{\\partial L}{\\partial b}=&amp;\\sum_{i=1}^{n}2(wx_i+b-y_i)=0\\&amp;\\Rightarrow b=\\frac{\\sum_{i=1}^{n}y_i-wx_i}{n}\\\\frac{\\partial L}{\\partial w}=&amp;2\\sum_{i=1}^{n}w(x_i)^2+(\\overline y-w\\overline x)x_i-x_iy_i\\&amp;=2\\sum_{i=1}^{n}[w(x_i)^2+\\overline y x_i - w \\overline x x_i -x_iy_i]=0\\&amp;\\Rightarrow w=\\frac{\\sum_{i=1}^{n}[x_iy_i-x_i\\overline y]}{\\sum_{i=1}^{n} [(x_i)^2-\\overline x x_i]}=\\frac{\\sum_{i=1}^{n}x_iy_i-\\frac{1}{n}\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n} y_i}{\\sum_{i=1}^{n} (x_i)^2-\\frac{1}{n}\\sum_{i=1}^{n} x_i \\sum_{i=1}^{n}x_i}\\&amp;=\\frac{\\sum_{i=1}^{n}(x_i-\\overline x)(y_i-\\overline y)}{\\sum_{i=1}^{n}(x_i-\\overline x)^2}\\\\end{aligned} 注意: 2.1 最小二乘法 对该模型最原始计算方法为最小二乘法，给定训练数据如下 则有 2.1.1 最小二乘法的概率角度的视角 设误差服从正态分布，那么有~，则有~, 公式建模 极大似然估计： 3. 优化 优化一般有两种，一种是正规方程组，一种是梯度下降法。已知损失函数 正规方程组: 对损失函数求导，即有, 令该式子为0则得 梯度下降：选择批量大小为, 那么有 正规方程组需要保证没有特征之间没有相关性，另外也无法应用于大规模的线性回归，而梯度下降需要选择合适的学习率，适合大规模场景下的线性回归，同时，随机梯度下降无法保证得到的解一定是最优，但正规方程组可以保证解为最优。 4. 拓展 4.1 岭回归 由于正规方程组需要保证可逆，对于有个样本，有个属性的数据，如果，那么不可逆。岭回归通过增加L2正则化来保证可逆。具体来说损失函数如下 加入的正则化有两个效果，一个是保证可逆，另一个是抑制过拟合。 如果样本数据过少导致线性回归拟合较差，则考虑采用岭回归。如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小. L2范数是各参数的平方和再求平方根，我们让L2范数的正则项最小，可以使的每个元素都很小，都接近于0。但它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。L2不能控制feature的“个数”，但是能防止模型overfit到某个feature上 4.2 Lasso回归 Lasso 回归的本质是 线性回归 + L1 正则化。 L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0, L1正则化是L0正则化的最优凸近似，比L0容易求解，并且也可以实现稀疏的效果, L1是控制feature“个数”的，并且鼓励模型在少量几个feature上有较大的权重。 4.3 局部加权线性回归 在线性回归中， 由于最终拟合出来的曲线是一条直线，其拟合能力极为有限（也可以解释为线性回归所求的是具有最小均方误差的无偏估计），因此很容易造成欠拟合现象， 而针对这个问题，有人提出了局部线性回归(LWR)。 在LWR中， 其损失函数为： 矩阵表示 此时，使用回归方程求得： 而通常， 服从高斯分布， 在目标区域附近指数型衰减; 其中， k 值越小，敏感半径越小。值决定了线性回归的拟合半径 4.3 ElasticNet回归 ElasticNet回归本质上是线性回归 + L1正则化 + L2 正则化，损失函数变为： Reference","categories":[],"tags":[{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"linear regression","slug":"linear-regression","permalink":"http://yourthomaslee.github.io/tags/linear-regression/"}]},{"title":"Leetcode 1604. Alert Using Same Key-Card Three or More Times in a One Hour Period","slug":"Leetcode 1604","date":"2023-02-07T02:40:05.000Z","updated":"2023-02-07T15:10:40.190Z","comments":true,"path":"2023/02/07/Leetcode 1604/","link":"","permalink":"http://yourthomaslee.github.io/2023/02/07/Leetcode%201604/","excerpt":"","text":"Problem description can be found at link My solution: 12345678910111213141516171819202122232425262728293031class Solution {public: struct op{ string name; int time; }; static bool comp(op&amp; a, op&amp; b){ if(a.name &lt; b.name){ return true; }else if(a.name == b.name){ return a.time &lt; b.time; }else return false; } vector&lt;string&gt; alertNames(vector&lt;string&gt;&amp; keyName, vector&lt;string&gt;&amp; keyTime) { vector&lt;op&gt; x; op t; for(int i = 0; i &lt; keyName.size(); ++i){ t.name = keyName[i]; t.time = stoi(keyTime[i].substr(0, 2)) * 60 + stoi(keyTime[i].substr(3, 2)); x.emplace_back(t); } sort(x.begin(), x.end(), comp); vector&lt;string&gt; ans; for(int i = 2; i &lt; x.size(); ++i){ if(ans.size() &gt; 0 &amp;&amp; x[i].name == ans[ans.size() - 1]) continue; if(x[i].name == x[i - 2].name &amp;&amp; abs(x[i].time - x[i - 2].time) &lt;= 60) ans.emplace_back(x[i].name); } return ans; }};","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yourthomaslee.github.io/tags/leetcode/"}]},{"title":"Note - An attention free transformer","slug":"Note-An attention free transformer","date":"2023-01-17T02:40:05.000Z","updated":"2023-02-07T15:08:41.515Z","comments":true,"path":"2023/01/17/Note-An attention free transformer/","link":"","permalink":"http://yourthomaslee.github.io/2023/01/17/Note-An%20attention%20free%20transformer/","excerpt":"","text":"Paper address: https://arxiv.org/pdf/2105.14103.pdf Related codes: https://github.com/BlinkDL/RWKV-LM Have not updated yet Concepts: a model with parameter training on dataset using stochastic gradient descent(SGD) : a batch of size from dataset at step with is the predictive distribution of the current model, where is the sequence of data the model was trained on before training step $ D_{ho} ={ (x_{ho}, y_{ho}) } {i=1}^{n{ho} } x_{ho}y_{ho}p_{true}(x′, y′)D$. Intuition: Previous online batch selection methods aim to select points that minimize the training set loss. Instead, we aim to select points that minimize the loss on a holdout set. We aim to acquire the point that would minimize the negative log-likelihood/cross-entropy loss on the holdout set: $ arg \\min {(x,y)\\in B_t} - \\log p(y{ho}|x_{ho};D_t \\cup (x, y))$ [get similar distribution as holdout set] Inference: For a model using a point estimate of (such as an MLE or MAP), rather than a distribution over , the holdout loss factorises and (up to a constant factor) forms a Monte Carlo approximation of the expected loss under : , where denotes the cross-entropy loss: , $$ \\log p(y_{ho}|x_{ho};D_t \\cup (x, y)) = \\log \\frac{p(y|x, y_{ho},x_{ho};D_t \\cup (x, y)) ; p(y_{ho}|x_{ho},x;D_t)}{p(y|x,x_{ho};D_t)}[Bayes; rule]\\ =\\log \\frac{p(y|x, y_{ho},x_{ho};D_t \\cup (x, y)) ; p(y_{ho}|x_{ho},x;D_t)}{p(y|x;D_t)}[conditional; independence; rule]\\ \\propto L[y|x;D_t] - L[y|x;D_{ho},D_t]\\ \\sim L[y|x;D_t] - L[y|x;D_{ho}]\\ \\Rightarrow \\arg \\max {(x,y) \\in B_t} L[y|x;D_t] - L[y|x;D{ho}]\\quad[training; loss - ;irreducible; holdout; loss] $$ Algorithm: Exploring: why not design a new simpler loss based on rough set thoery? it seems RHOLOSS is still a little expensive in implementation and computation. Actually this work provides a good way to think about the training sample selection problem. There are still room for some more further researches in computation simplicity and cost aspects","categories":[],"tags":[{"name":"transfomer","slug":"transfomer","permalink":"http://yourthomaslee.github.io/tags/transfomer/"},{"name":"language model","slug":"language-model","permalink":"http://yourthomaslee.github.io/tags/language-model/"}]},{"title":"Curriculum vitae - Baizhen Li","slug":"Curriculum Vitae - Baizhen Li","date":"2022-12-26T13:54:13.432Z","updated":"2023-02-10T04:23:14.229Z","comments":true,"path":"2022/12/26/Curriculum Vitae - Baizhen Li/","link":"","permalink":"http://yourthomaslee.github.io/2022/12/26/Curriculum%20Vitae%20-%20Baizhen%20Li/","excerpt":"","text":"Contact information: Bricklees [at] alumni [dot] tongji [dot] edu [dot] cn || baizhen9406 [at] 163 [dot] com Education Tongji UNIVERSITY 06/2018 - 03/2021 M.E., Computer Technology. GPA: 87/100 Thesis: Research on dialogue state tracking algorithm based on cross-layer fusion Awards and honors: National Scholarship(2020), Second Prize of the 15th China Post-graduate Mathematical Contest in Modelling, Outstanding Dissertation Award YANTAI UNIVERSITY 06/2013 - 07/2017 B.E., Computer Science and Technology. GPA: 79/100 Thesis: Research on knowledge acquisition algorithms: a discernibility matrix approach Awards and honors: Outstanding Dissertation Award, Third Prize of LAN QIAO International Collegiate Programming Contest Experiences I. Research experiences From 08/2019 to 12/2020, I participated in “Research on the Refined Description and Interpretability of Targets under Surveillance Video”(National natural science foundation of China, No. 61976160). I was in charge of models that provide task-oriented description of a picture collected from surveillance video while working as a researcher under the direction of professor Zhihua Wei and associate professor Lijun Sun. From 06/2018 to 07/2019, I took part in \"Topic Analysis Technology based on Natural Language Processing” (application research cooperated with the key lab of information network security, China ministry of public security, No. C18608). I created topic analysis-based detection techniques for illicit websites detection while working as a researcher and developer under the direction of professor Zhihua Wei and associate professor Lijun Sun. From 03/2014 to 06/2017, I participated in “Knowledge Space Research based on Granular Computing Method” (national natural science foundation of China youth science fund project, No. 61403329). I created two more effective rough sets-based knowledge discovery algorithms as a research assistant under the direction of associate professor Nan Zhang. II. Work/Internship experiences BAIDU CHINA CO., LTD. 03/2021 – 05/2022 As an engineer, I was in charge of the real-time bidding system’s cost-per-thousand-impression calibration(CTIA) module. By tweaking the algorithm’s parameters and state estimates, I improved the proportional-integral-derivative (PID) control algorithm’s performance in CTIA. HAIYIZHI INFORMATION TECHNOLOGY CO., LTD. 06/2020 – 09/2020 I created a robot that serves as the customer support representative in the postal service’s online chat. After that, I used a non-autoregressive dialog state tracking model to reduce the inference latency for dialog state tracking. Publications [1] Baizhen Li, Yibin Zhan, Zhihua Wei, Shikun Huang, Lijun Sun: Improved non-autoregressive dialog state tracking model. CCRIS 2021: 199-203 [2] Baizhen Li, Zhihua Wei, Duoqian Miao, Nan Zhang, Wen Shen, Chang Gong, Hongyun Zhang, Lijun Sun: Improved general attribute reduction algorithms. Inf. Sci. 536: 298-316 (2020) [3] Baizhen Li, Wei Chen, Zhihua Wei, Hongyun Zhang, Nan Zhang, Lijun Sun: Quick Maximum Distribution Reduction in Inconsistent Decision Tables. IJCRS 2020: 169-182 [4] Nan Zhang, Baizhen Li, Zhongxi Zhang, Yanyan Guo: A Quick Algorithm for Binary Discernibility Matrix Simplification using Deterministic Finite Automata. Inf. 9(12): 314 (2018)","categories":[],"tags":[]},{"title":"Note - Prioritized training on points that are learnable worth learning and not yet learnt","slug":"Note-Prioritized-training-on-points-that-are-learnable-worth-learning-and-not-yet-learnt","date":"2022-07-22T02:40:05.000Z","updated":"2022-12-27T03:36:11.314Z","comments":true,"path":"2022/07/22/Note-Prioritized-training-on-points-that-are-learnable-worth-learning-and-not-yet-learnt/","link":"","permalink":"http://yourthomaslee.github.io/2022/07/22/Note-Prioritized-training-on-points-that-are-learnable-worth-learning-and-not-yet-learnt/","excerpt":"","text":"This paper introduce RHO_LOSS, which selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT) Code: https://github.com/OATML/RHO-Loss Concepts: a model with parameter training on dataset using stochastic gradient descent(SGD) : a batch of size from dataset at step with is the predictive distribution of the current model, where is the sequence of data the model was trained on before training step $ D_{ho} ={ (x_{ho}, y_{ho}) } {i=1}^{n{ho} } x_{ho}y_{ho}p_{true}(x′, y′)D$. Intuition: Previous online batch selection methods aim to select points that minimize the training set loss. Instead, we aim to select points that minimize the loss on a holdout set. We aim to acquire the point that would minimize the negative log-likelihood/cross-entropy loss on the holdout set: $ arg \\min {(x,y)\\in B_t} - \\log p(y{ho}|x_{ho};D_t \\cup (x, y))$ [get similar distribution as holdout set] Inference: For a model using a point estimate of (such as an MLE or MAP), rather than a distribution over , the holdout loss factorises and (up to a constant factor) forms a Monte Carlo approximation of the expected loss under : , where denotes the cross-entropy loss: , $$ \\log p(y_{ho}|x_{ho};D_t \\cup (x, y)) = \\log \\frac{p(y|x, y_{ho},x_{ho};D_t \\cup (x, y)) ; p(y_{ho}|x_{ho},x;D_t)}{p(y|x,x_{ho};D_t)}[Bayes; rule]\\ =\\log \\frac{p(y|x, y_{ho},x_{ho};D_t \\cup (x, y)) ; p(y_{ho}|x_{ho},x;D_t)}{p(y|x;D_t)}[conditional; independence; rule]\\ \\propto L[y|x;D_t] - L[y|x;D_{ho},D_t]\\ \\sim L[y|x;D_t] - L[y|x;D_{ho}]\\ \\Rightarrow \\arg \\max {(x,y) \\in B_t} L[y|x;D_t] - L[y|x;D{ho}]\\quad[training; loss - ;irreducible; holdout; loss] $$ Algorithm: Exploring: why not design a new simpler loss based on rough set thoery? it seems RHOLOSS is still a little expensive in implementation and computation. Actually this work provides a good way to think about the training sample selection problem. There are still room for some more further researches in computation simplicity and cost aspects","categories":[],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://yourthomaslee.github.io/tags/deep-learning/"},{"name":"training sample selection","slug":"training-sample-selection","permalink":"http://yourthomaslee.github.io/tags/training-sample-selection/"}]},{"title":"Note-Trainable learning rate","slug":"Note-Trainable-learning-rate","date":"2022-07-19T09:26:53.000Z","updated":"2022-12-27T03:38:45.291Z","comments":true,"path":"2022/07/19/Note-Trainable-learning-rate/","link":"","permalink":"http://yourthomaslee.github.io/2022/07/19/Note-Trainable-learning-rate/","excerpt":"","text":"Problem: selecting an appropriate learning rate is a challenge considering different model structures and datasets. Work: we propose an algorithm for automatically adjusting the learning rate during the gradient descent (line search method) Given the weights’ gradients, one could estimate if an overestimation or an underestimation of learning rate could further reduce the task loss by casting the learning rate as an extra trainable parameter. Problem Statement: the gradient descent framework (GD). vanilla form: , where is learning rate, is loss function and is the model parameters at iteration . In vanilla GD, Is treated as a hyperparameter and does not contribute to the loss. We introduce an augmented loss term , where we consider as the learnable variable. . Something noticing is that minimizing is equivalent to finding the optimal step-size for current . Naive GD-TLR: A straightforward idea is to apply GD over : , where hyperparameter Controls the updates of the initial learning rate Disadvantages: the addition of requires an extra forward-backward pass in order to compute the introduction of a GD step for creates the extra hyperparameter Efficient GD-TLR: let Denotes a standard feed-forward network of Layers, where and denote the non-linear activation and linear transformation corresponding to layer respectively. For the convenience of writing, we just consider the linearity of a single layer , where is the input from the previous layer, is the matrix of parameters, is the output. (, ) denotes the variables after GD update. So we have First-order gradient and insight: Assuming that corresponds to a specific layer, the gradient of the augmented loss compute as: the learning rate gradient can be expressed as the inner product of consecutive gradients , as a result we can rewrite equation (2) as $ \\frac{\\partial L_\\alpha}{\\partial \\alpha}|{a=\\alpha_t} = -&lt;\\triangledown L(w_t), \\triangledown L(w{t - 1})&gt; = -&lt;g_t, g_{t-1}&gt; $. The derived gradient (take indicator ind for simplifing) has an intuitive interpretation: Ind &gt; 0: the learning rate should be increased Ind &lt; 0: the learning rate should be decreased Ind = 0: either we reached a converged state of gradient directions are perpendicular. Second-order gradient: a newton-based method requires Hessian computations/approximations of the network’s weights, and the problem at hand has an intuitive analytical form, using only first-order weight gradients, as following suggests: $$ \\frac{\\partial^2L}{\\partial \\alpha^2}|{\\alpha=\\alpha_t}=\\frac{4}{\\alpha}&lt;g_t, g_t-g{t-1}&gt;=\\frac{4}{\\alpha_t}(||g_t||^2-&lt;g_t,g_{t-1}&gt;) \\eta_t = \\frac{\\alpha_t}{\\max(4&lt;g_t, g_t-g_{t-1}), c^{-1}&lt;g_t, g_{t-1}&gt;)} $$ where we form an overall bound on the update of which imposes smoother behavior (c was set as 1/4 in paper) Efficient GD-TLR Input: number of iterations , initial weights , learning rate , hyperparameter Output: optimized weights step 1: initialize = 0 step 2: for to do step 3: single forward-backward pass: step 4: compute step 5: update alpha according to Equation 2 and 3 [ ] step 6: update w: ; step 7: ; step 8: end for Exploring problem： the analysis in appendices, may be we will explore it next time I read it. Interested in automatical layer-wise learning rate in the future research the performance and the disadvantage of the algorithm. I think there is still necessity of experiment","categories":[],"tags":[{"name":"trainable learning rate","slug":"trainable-learning-rate","permalink":"http://yourthomaslee.github.io/tags/trainable-learning-rate/"}]},{"title":"Notes-mitigating neural network overconfidence with logit normalization","slug":"Note-mitigating neural network overconfidence with logit-normalization","date":"2022-07-11T08:49:33.000Z","updated":"2022-12-27T03:34:16.757Z","comments":true,"path":"2022/07/11/Note-mitigating neural network overconfidence with logit-normalization/","link":"","permalink":"http://yourthomaslee.github.io/2022/07/11/Note-mitigating%20neural%20network%20overconfidence%20with%20logit-normalization/","excerpt":"introduce Logit Normalization (LogitNorm), a simple fix to the cross-entropy loss (by enforcing a constant vector norm on the logits in training), to deal with overfitting or overconfidence for both in- and out-of-distribution inputs.","text":"introduce Logit Normalization (LogitNorm), a simple fix to the cross-entropy loss (by enforcing a constant vector norm on the logits in training), to deal with overfitting or overconfidence for both in- and out-of-distribution inputs. Problem: we find that even when most training examples are classified to their correct labels, the softmax cross-entropy loss can continue to increase the magnitude of the logit vectors. The growing magnitude during training thus leads to the overconfidence issue, despite having no improvement on the classification accuracy. Work: To mitigate the issue, our key idea behind LogitNorm is to decouple the influence of output’s norm from the training objective and its optimization. This can be achieved by normalizing the logit vector to have a constant norm dur- ing training. In effect, our LogitNorm loss encourages the direction of the logit output to be consistent with the corre- sponding one-hot label, without exacerbating the magnitude of the output. Trained with normalized outputs, the network tends to give conservative predictions and results in strong separability of softmax confidence scores between ID and OOD inputs Codes: https://github.com/hongxin001/logitnorm_ood Core equation: to make sure that the logit vector is a unit vector, it alleviate some challenges in optimization, or rather, let optimization does more things useful! where is logit vector, is the Euclidean norm of the logit vector, and denotes the quantity of different class labels. Comment: This work restricts the numerical bound of logit vector during optimization, and it results better model performance in classification task. Exploring problem： Can we get the better performance if we use label smoothing? what is the difference between LayerNorm and LogitNorm? Is there any other efficient way to implement LogitNorm? Is this work compatible to the paper “Long-Tail Learning via Logit Adjustment” (https://arxiv.org/pdf/2007.07314.pdf)?","categories":[],"tags":[{"name":"overfitting","slug":"overfitting","permalink":"http://yourthomaslee.github.io/tags/overfitting/"},{"name":"logit normalization","slug":"logit-normalization","permalink":"http://yourthomaslee.github.io/tags/logit-normalization/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-03-20T15:33:35.180Z","updated":"2022-03-20T15:33:34.000Z","comments":true,"path":"2022/03/20/hello-world/","link":"","permalink":"http://yourthomaslee.github.io/2022/03/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"deep learning framework","slug":"deep-learning-framework","permalink":"http://yourthomaslee.github.io/tags/deep-learning-framework/"},{"name":"machine learning models","slug":"machine-learning-models","permalink":"http://yourthomaslee.github.io/tags/machine-learning-models/"},{"name":"Markov chain","slug":"Markov-chain","permalink":"http://yourthomaslee.github.io/tags/Markov-chain/"},{"name":"Condition random field","slug":"Condition-random-field","permalink":"http://yourthomaslee.github.io/tags/Condition-random-field/"},{"name":"model enhancement","slug":"model-enhancement","permalink":"http://yourthomaslee.github.io/tags/model-enhancement/"},{"name":"perception machine","slug":"perception-machine","permalink":"http://yourthomaslee.github.io/tags/perception-machine/"},{"name":"support vector machine","slug":"support-vector-machine","permalink":"http://yourthomaslee.github.io/tags/support-vector-machine/"},{"name":"logic regression","slug":"logic-regression","permalink":"http://yourthomaslee.github.io/tags/logic-regression/"},{"name":"leetcode","slug":"leetcode","permalink":"http://yourthomaslee.github.io/tags/leetcode/"},{"name":"dynamic programming","slug":"dynamic-programming","permalink":"http://yourthomaslee.github.io/tags/dynamic-programming/"},{"name":"combination","slug":"combination","permalink":"http://yourthomaslee.github.io/tags/combination/"},{"name":"interesting problems","slug":"interesting-problems","permalink":"http://yourthomaslee.github.io/tags/interesting-problems/"},{"name":"maximum entropy model","slug":"maximum-entropy-model","permalink":"http://yourthomaslee.github.io/tags/maximum-entropy-model/"},{"name":"hash","slug":"hash","permalink":"http://yourthomaslee.github.io/tags/hash/"},{"name":"deep learning concepts","slug":"deep-learning-concepts","permalink":"http://yourthomaslee.github.io/tags/deep-learning-concepts/"},{"name":"activation function","slug":"activation-function","permalink":"http://yourthomaslee.github.io/tags/activation-function/"},{"name":"array","slug":"array","permalink":"http://yourthomaslee.github.io/tags/array/"},{"name":"machine learning framework","slug":"machine-learning-framework","permalink":"http://yourthomaslee.github.io/tags/machine-learning-framework/"},{"name":"linear regression","slug":"linear-regression","permalink":"http://yourthomaslee.github.io/tags/linear-regression/"},{"name":"transfomer","slug":"transfomer","permalink":"http://yourthomaslee.github.io/tags/transfomer/"},{"name":"language model","slug":"language-model","permalink":"http://yourthomaslee.github.io/tags/language-model/"},{"name":"deep learning","slug":"deep-learning","permalink":"http://yourthomaslee.github.io/tags/deep-learning/"},{"name":"training sample selection","slug":"training-sample-selection","permalink":"http://yourthomaslee.github.io/tags/training-sample-selection/"},{"name":"trainable learning rate","slug":"trainable-learning-rate","permalink":"http://yourthomaslee.github.io/tags/trainable-learning-rate/"},{"name":"overfitting","slug":"overfitting","permalink":"http://yourthomaslee.github.io/tags/overfitting/"},{"name":"logit normalization","slug":"logit-normalization","permalink":"http://yourthomaslee.github.io/tags/logit-normalization/"}]}